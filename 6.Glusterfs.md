#分布式文件系统glusterFS
<pre>
###windows远程桌面无法复制粘添：在目标主机上杀死rdpclip进程并重新运行rdpclip.exe即可

事前准备：
1. 准备4台虚拟机，并设定主机名和ip,主机名要想即时生效，可先设临时主机名以后再重启即可
2. 关闭selinux和防火墙
3. 给每台虚拟机再加一块10G硬盘，并使用  echo "- - -" > /sys/class/scsi_host/host${i}/scan 进行添加的硬盘即时生效
------- 添加新硬盘即时生效脚本
[root@salt-server ~]# cat disk.tmp
#!/usr/bin/bash

scsisum=`ls -l /sys/class/scsi_host/host*|wc -l`

for ((i=0;i<${scsisum};i++))
do
    echo "- - -" > /sys/class/scsi_host/host${i}/scan
done
------

开始安装：
1. 安装epel源：
[root@salt-server ~]# salt 'clus*' cmd.run "yum install -y epel-release"
2. 安装gluster源：
[root@salt-server ~]# salt 'clusterFS-node*' cmd.run "yum install -y centos-release-gluster41.noarch"
3. 安装glusterfs:
[root@salt-server ~]# salt 'clusterFS-node*' cmd.run "yum --enablerepo=centos-gluster*-test install glusterfs-server glusterfs-cli glusterfs-geo-replication -y"
4. 查看安装的gluster包：
clusterFS-node1-salt:
    centos-release-gluster41-1.0-3.el7.centos.noarch
    glusterfs-client-xlators-4.1.6-1.el7.x86_64
    glusterfs-4.1.6-1.el7.x86_64
    glusterfs-api-4.1.6-1.el7.x86_64
    glusterfs-fuse-4.1.6-1.el7.x86_64
    python2-gluster-4.1.6-1.el7.x86_64
    glusterfs-geo-replication-4.1.6-1.el7.x86_64
    glusterfs-libs-4.1.6-1.el7.x86_64
    glusterfs-cli-4.1.6-1.el7.x86_64
    glusterfs-server-4.1.6-1.el7.x86_64
5. 配置glusterfs:
  1. 查看glusterfs版本
  [root@salt-server ~]# salt clusterFS-node* cmd.run 'glusterfs -V'
  clusterFS-node2-salt:
    glusterfs 4.1.6
    Repository revision: git://git.gluster.org/glusterfs.git
    Copyright (c) 2006-2016 Red Hat, Inc. <https://www.gluster.org/>
    GlusterFS comes with ABSOLUTELY NO WARRANTY.
    It is licensed to you under your choice of the GNU Lesser
    General Public License, version 3 or any later version (LGPLv3
    or later), or the GNU General Public License, version 2 (GPLv2),
    in all cases as published by the Free Software Foundation.
  2. 启动glusterd服务
    2.1 [root@salt-server ~]# salt clusterFS-node* cmd.run 'systemctl start glusterd'
clusterFS-node3-salt:
clusterFS-node2-salt:
clusterFS-node4-salt:
clusterFS-node1-salt:
    2.2 [root@salt-server ~]# salt clusterFS-node* cmd.run 'systemctl enable glusterd' #设置开机启动
clusterFS-node4-salt:
    Created symlink from /etc/systemd/system/multi-user.target.wants/glusterd.service to /usr/lib/systemd/system/glusterd.service.
    2.3 [root@salt-server ~]# salt clusterFS-node* service.status glusterd #查看服务状态
clusterFS-node3-salt:
    True
clusterFS-node4-salt:
    True
clusterFS-node1-salt:
    True
clusterFS-node2-salt:
    True
  3. 存储主机加入信任存储池中（整合磁盘）：在任意其中一台gluster中添加其他的gluster到信任存储池中：
  [root@clusterFS-node1-salt ~]# gluster peer probe clusterFS-node2-salt.jack.com
peer probe: success.
[root@clusterFS-node1-salt ~]# gluster peer probe clusterFS-node3-salt.jack.com
peer probe: success.
[root@clusterFS-node1-salt ~]# gluster peer probe clusterFS-node4-salt.jack.com
peer probe: success.
  4. 任意一台gluster中查看其他gluster的状态：
  [root@clusterFS-node2-salt ~]# gluster peer status
Number of Peers: 3

Hostname: 192.168.1.32
Uuid: be55a468-9d4f-4211-961d-2cfbd9d9aa6d
State: Peer in Cluster (Connected)

Hostname: clusterFS-node3-salt.jack.com
Uuid: 1ffc6e81-af6f-448d-8579-fc69761280f7
State: Peer in Cluster (Connected)

Hostname: clusterFS-node4-salt.jack.com
Uuid: 0b4e5407-ec4e-40e9-b41c-fbb46086bc12
State: Peer in Cluster (Connected)
  5. cent0s6安装xfs文件系统，对添加的硬盘进行xfs格式化:yum install -u xfsprogs,centos7系统自带可不用安装(因为ext4文件系统最大支持16TB，而xfs文件系统支持PB级。)
  6. 官网文档对新硬盘要进行分一个区，实际操作不进行分区也没什么问题。系统盘要做RAID,gluster数据盘不用做RAID
  7. 使用xfs格式化硬盘：[root@salt-server ~]# salt clusterFS-node* cmd.run 'mkfs.xfs /dev/sdb'
  8. [root@salt-server ~]# salt clusterFS-node* cmd.run 'mkdir -p /storage/brick1 && mount /dev/sdb /storage/brick1'
  9. 查看挂载情况：
  [root@salt-server ~]# salt clusterFS-node* cmd.run 'df -h'                      clusterFS-node4-salt:
    Filesystem      Size  Used Avail Use% Mounted on
    /dev/sda2        15G  1.6G   14G  11% /
    devtmpfs        909M     0  909M   0% /dev
    tmpfs           920M   12K  920M   1% /dev/shm
    tmpfs           920M  8.8M  911M   1% /run
    tmpfs           920M     0  920M   0% /sys/fs/cgroup
    /dev/sda1      1014M  140M  875M  14% /boot
    tmpfs           184M     0  184M   0% /run/user/0
    /dev/sdb         10G   33M   10G   1% /storage/brick1
  10. [root@salt-server ~]# salt  clusterFS-node* cmd.run 'echo /dev/sdb /storage/brick1 xfs defaults 0 0 >> /etc/fstab'
  11. [root@salt-server ~]#  salt clusterFS-node* cmd.run 'mount -a'
  12. 分布式文件系统卷：
    1. 分布卷
    2. 复制卷
    3. 条带卷
    4. 分布式条带卷（服务器必须是2的倍数）
    5. 分布式复制卷（服务器必须是2的倍数）这种卷用得最多
  13. 创建分布卷：
  [root@clusterFS-node1-salt ~]# gluster volume create gv1 clusterFS-node1-salt.jack.com:/storage/brick1 clusterFS-node2-salt.jack.com:/storage/brick1 force
  volume create: gv1: success: please start the volume to access data
  14. 启动卷：[root@clusterFS-node1-salt ~]# gluster volume start gv1
  volume start: gv1: success
  （创建gluster1和gluster2的gv1卷，在gluster3和其他都可以查看到）
  15. [root@salt-server ~]# salt clusterFS-node3* cmd.run 'gluster volume info' #其他gluster上查看卷的信息
clusterFS-node3-salt:
    Volume Name: gv1
    Type: Distribute  #分布式的卷
    Volume ID: 0c530420-b9fc-4381-8668-2a316a7c6509
    Status: Started
    Snapshot Count: 0
    Number of Bricks: 2
    Transport-type: tcp
    Bricks:
    Brick1: clusterFS-node1-salt.jack.com:/storage/brick1
    Brick2: clusterFS-node2-salt.jack.com:/storage/brick1
    Options Reconfigured:
    transport.address-family: inet
    nfs.disable: on
  16. 信任池中任意一台gluster都可挂载：
[root@clusterFS-node3-salt ~]# mount -t glusterfs 127.0.0.1:/gv1 /mnt
  17. 查看挂载情况：
[root@clusterFS-node3-salt ~]# df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/sda2        15G  1.6G   14G  11% /
devtmpfs        909M     0  909M   0% /dev
tmpfs           920M   12K  920M   1% /dev/shm
tmpfs           920M  8.8M  911M   1% /run
tmpfs           920M     0  920M   0% /sys/fs/cgroup
/dev/sda1      1014M  140M  875M  14% /boot
/dev/sdb         10G   33M   10G   1% /storage/brick1
tmpfs           184M     0  184M   0% /run/user/0
127.0.0.1:/gv1   20G  270M   20G   2% /mnt
  18. [root@clusterFS-node1-salt ~]# mount -t glusterfs 127.0.0.1:/gv1 /mnt
  19. 任意一台gluster服务器挂载卷后都可能卷进行文件拷坝，而且每台gluster服务器都会进行同步数据
  20. 用nfs方式挂载gluster卷（centos7不支持）：mount -t nfs -o mountproto=tcp 192.168.1.37:/gv1 /mnt
  21. 创建复制卷：
[root@clusterFS-node1-salt ~]# gluster volume create gv2 replica 2 clusterFS-node3-salt.jack.com:/storage/brick1 clusterFS-node4-salt.jack.com:/storage/brick1 force #replica为复制卷，后面的2为复制两个，如果复制3个则后面为3
volume create: gv2: success: please start the volume to access data
  22. 查看所有卷：
  [root@clusterFS-node1-salt ~]# gluster volume info
Volume Name: gv1
Type: Distribute
Volume ID: 0c530420-b9fc-4381-8668-2a316a7c6509
Status: Started
Snapshot Count: 0
Number of Bricks: 2
Transport-type: tcp
Bricks:
Brick1: clusterFS-node1-salt.jack.com:/storage/brick1
Brick2: clusterFS-node2-salt.jack.com:/storage/brick1
Options Reconfigured:
transport.address-family: inet
nfs.disable: on

Volume Name: gv2
Type: Replicate
Volume ID: 34f461e7-91ec-496f-a6d4-e48ba5fc08ac
Status: Created
Snapshot Count: 0
Number of Bricks: 1 x 2 = 2
Transport-type: tcp
Bricks:
Brick1: clusterFS-node3-salt.jack.com:/storage/brick1
Brick2: clusterFS-node4-salt.jack.com:/storage/brick1
Options Reconfigured:
transport.address-family: inet
nfs.disable: on
performance.client-io-threads: off
  23. 启动复制卷gv2:
  [root@clusterFS-node1-salt ~]# gluster volume start gv2
volume start: gv2: success
  24. 挂载：
  [root@clusterFS-node1-salt ~]# mount -t glusterfs 127.0.0.1:/gv2 /opt
  25. 查看挂载情况：
  [root@clusterFS-node1-salt ~]# df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/sda2        15G  1.6G   14G  11% /
devtmpfs        909M     0  909M   0% /dev
tmpfs           920M   12K  920M   1% /dev/shm
tmpfs           920M  8.9M  911M   1% /run
tmpfs           920M     0  920M   0% /sys/fs/cgroup
/dev/sda1      1014M  140M  875M  14% /boot
tmpfs           184M     0  184M   0% /run/user/0
/dev/sdb         10G   33M   10G   1% /storage/brick1
127.0.0.1:/gv1   20G  270M   20G   2% /mnt
127.0.0.1:/gv2   10G  135M  9.9G   2% /opt
！！注意：当挂载卷时目录挂载错了，需要更换目录时，最好是把之前的硬盘重新格式化重新建卷重新挂载，因为挂卷后有gluster的配置隐藏文件，再重新挂载会影响以后的gluster使用。 
{
危险操作：
删除卷并重新格式化硬盘：
1. [root@clusterFS-node1-salt ~]# gluster volume stop gv1 #停止卷的工作
Stopping volume will make its data inaccessible. Do you want to con              y
volume stop: gv1: success
2. [root@clusterFS-node1-salt ~]# gluster volume delete gv1  #删除卷
Deleting volume will erase all information about the volume. Do you              inue? (y/n) y
volume delete: gv1: success
3. [root@salt-server ~]# salt clusterFS-node* cmd.run 'umount /dev/sdb'  #卸载硬盘
4. [root@salt-server ~]# salt clusterFS-node* cmd.run 'mkfs -t xfs -f /dev/sdb '  #格式化sdb硬盘
5. [root@salt-server ~]# salt clusterFS-node* cmd.run 'mount /dev/sdb /storage/brick1 '  #重新挂载
clusterFS-node1-salt:
clusterFS-node3-salt:
clusterFS-node4-salt:
clusterFS-node2-salt:
}

##分布式条带卷
#条带卷：
1. [root@clusterFS-node1-salt ~]# gluster volume create gv1 stripe 2 clusterFS-node1-salt.jack.com:/storage/brick1 clusterFS-node2-salt.jack.com:/storage/brick1 force #新建条带卷
volume create: gv1: success: please start the volume to access data
2. [root@clusterFS-node1-salt ~]# gluster volume start gv1 #启动卷
volume start: gv1: success
查看条带卷：
[root@clusterFS-node1-salt ~]# gluster volume info
Volume Name: gv1
Type: Stripe
Volume ID: bc0219b4-8967-4928-9a17-226fe86d2a90
Status: Started
Snapshot Count: 0
Number of Bricks: 1 x 2 = 2
Transport-type: tcp
Bricks:
Brick1: clusterFS-node1-salt.jack.com:/storage/brick1
Brick2: clusterFS-node2-salt.jack.com:/storage/brick1
Options Reconfigured:
transport.address-family: inet
nfs.disable: on

3. 挂载卷：
[root@salt-server ~]# salt clusterFS-node* cmd.run "mkdir /gv1 && mount -t glusterfs 127.0.0.1:gv1 /gv1  "
clusterFS-node2-salt:
clusterFS-node1-salt:
clusterFS-node4-salt:
clusterFS-node3-salt:
4. [root@clusterFS-node1-salt gv1]# dd if=/dev/zero bs=1024 count=10000 of=/gv1/10M
10000+0 records in
10000+0 records out
10240000 bytes (10 MB) copied, 1.26171 s, 8.1 MB/s
5. [root@clusterFS-node1-salt gv1]# ll -h
total 9.8M
-rw-r--r-- 1 root root 9.8M Jan 13 19:16 10M
6. 查看
[root@clusterFS-node1-salt gv1]# ll -h /storage/brick1/
total 4.9M
-rw-r--r-- 2 root root 4.9M Jan 13 19:16 10M
[root@salt-server ~]# salt clusterFS-node2* cmd.run "ls -lh /storage/brick1 "
clusterFS-node2-salt:
    total 4.9M
    -rw-r--r-- 2 root root 4.9M Jan 13 19:16 10M
[root@salt-server ~]# salt clusterFS-node3* cmd.run "ls -lh /storage/brick1 "
clusterFS-node3-salt:
    total 0
[root@salt-server ~]# salt clusterFS-node4* cmd.run "ls -lh /storage/brick1 "
clusterFS-node4-salt:
    total 0

#分布式条带卷：
[root@clusterFS-node1-salt gv1]# gluster volume stop gv1
[root@clusterFS-node1-salt gv1]# gluster volume add-brick gv1 stripe 2 clusterFS-node3-salt.jack.com:/storage/brick1 clusterFS-node4-salt.jack.com:/storage/brick1 force #增加两个卷到gv1这个条带卷中使之成为分布式条带卷
 gluster volume remove-brick gv1 stripe 2 clusterFS-node3-salt.jack.com:/storage/brick1 clusterFS-node4-salt.jack.com:/storage/brick1 force


3. 查看分布式条带卷信息：
[root@clusterFS-node1-salt gv1]# gluster volume info
Volume Name: gv1
Type: Distributed-Stripe
Volume ID: bc0219b4-8967-4928-9a17-226fe86d2a90
Status: Started
Snapshot Count: 0
Number of Bricks: 2 x 2 = 4
Transport-type: tcp
Bricks:
Brick1: clusterFS-node1-salt.jack.com:/storage/brick1
Brick2: clusterFS-node2-salt.jack.com:/storage/brick1
Brick3: clusterFS-node3-salt.jack.com:/storage/brick1
Brick4: clusterFS-node4-salt.jack.com:/storage/brick1
Options Reconfigured:
transport.address-family: inet
nfs.disable: on

[root@clusterFS-node3-salt gv1]# df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/sda2        15G  1.6G   14G  11% /
devtmpfs        909M     0  909M   0% /dev
tmpfs           920M   12K  920M   1% /dev/shm
tmpfs           920M  8.8M  911M   1% /run
tmpfs           920M     0  920M   0% /sys/fs/cgroup
/dev/sda1      1014M  140M  875M  14% /boot
tmpfs           184M     0  184M   0% /run/user/0
/dev/sdb         10G   33M   10G   1% /storage/brick1
127.0.0.1:gv1    40G  559M   40G   2% /gv1

[root@salt-server ~]# salt clusterFS-node* cmd.run "ls -lh /storage/brick1"
clusterFS-node4-salt:
    total 0
clusterFS-node1-salt:
    total 9.8M
    -rw-r--r-- 2 root root 4.9M Jan 13 20:18 10M
    -rw-r--r-- 2 root root 4.9M Jan 13 20:29 10M-10
    -rw-r--r-- 2 root root    5 Jan 13 20:09 tripe.txt
clusterFS-node3-salt:
    total 0
clusterFS-node2-salt:
    total 9.8M
    -rw-r--r-- 2 root root 4.9M Jan 13 20:18 10M
    -rw-r--r-- 2 root root 4.9M Jan 13 20:29 10M-10
    -rw-r--r-- 2 root root    0 Jan 13 20:09 tripe.txt
！！注意从上面看出新增加的数据还是没有添加到新添加的卷中，需要做磁盘平衡后才能添加到新添加的卷中。

#磁盘平衡：
当你添加了卷到条带卷或者复制卷中组成了分布式的卷，都需要做磁盘平衡，否则新增加的或者以前的数据都不会同步到新增加的卷中。
磁盘平衡：
[root@clusterFS-node1-salt gv1]# gluster volume rebalance gv1 start
volume rebalance: gv1: success: Rebalance on gv1 has been started successfully. Use rebalance status command to check status of the rebalance process.
ID: 4635c875-75f3-4f04-9c9e-d6d7eaaa2ca3
查看硬盘平衡的工作状态：
[root@clusterFS-node1-salt gv1]# gluster volume rebalance gv1 status
                                    Node Rebalanced-files          size       scanned      failures       skipped               status  run time in h:m:s
                               ---------      -----------   -----------   -----------   -----------   -----------         ------------     --------------
                               localhost                2         9.8MB             3             0             0            completed        0:00:01
           clusterFS-node2-salt.jack.com                0        0Bytes             0             0             0            completed        0:00:00
           clusterFS-node3-salt.jack.com                0        0Bytes             1             0             0            completed        0:00:00
           clusterFS-node4-salt.jack.com                0        0Bytes             0             0             0            completed        0:00:00
volume rebalance: gv1: success

做完磁盘平衡后把原先的条带卷数据平均到另外的一个条带卷中：
[root@salt-server ~]# salt clusterFS-node* cmd.run "ls -lh /storage/brick1"
clusterFS-node4-salt:
    total 4.9M
    -rw-r--r-- 2 root root 4.9M Jan 13 20:18 10M
    -rw-r--r-- 2 root root    0 Jan 13 20:09 tripe.txt
clusterFS-node2-salt:
    total 4.9M
    -rw-r--r-- 2 root root 4.9M Jan 13 20:29 10M-10
clusterFS-node1-salt:
    total 4.9M
    -rw-r--r-- 2 root root 4.9M Jan 13 20:29 10M-10
clusterFS-node3-salt:
    total 4.9M
    -rw-r--r-- 2 root root 4.9M Jan 13 20:18 10M
    -rw-r--r-- 2 root root    5 Jan 13 20:09 tripe.txt

##分布式复制卷：
创建复制卷：
1. [root@clusterFS-node1-salt ~]# gluster volume create gv2 replica 2 clusterFS-node3-salt.jack.com:/storage/brick1 clusterFS-node4-salt.jack.com:/storage/brick1 force #replica为复制卷，后面的2为复制两个，如果复制3个则后面为3
启动复制卷gv2:
2. [root@clusterFS-node1-salt ~]# gluster volume start gv2
volume start: gv2: success
增加卷到复制卷中使之成为分布式复制卷：
3. [root@clusterFS-node1-salt ~]#gluster volume add-brick gv2 replica 2 clusterFS-node1-salt.jack.com:/storage/brick1 clusterFS-node2-salt.jack.com:/storage/brick1 force #增加两个卷到gv2这个条带卷中使之成为分布式复制卷
4. [root@clusterFS-node1-salt ~]# gluster volume rebalance gv2 start #磁盘平衡使gluster3,gluster4复制卷的数据平均一点到gluster1,gluster2复制卷中，
5. [root@clusterFS-node1-salt ~]# gluster volume rebalance gv2 status #可查看磁盘平衡的工作状态

移除卷：
1. [root@clusterFS-node1-salt ~]# gluster volume stop gv2
2. [root@clusterFS-node1-salt ~]#gluster volume remove-brick gv2 replica 2 clusterFS-node1-salt.jack.com:/storage/brick1 clusterFS-node2-salt.jack.com:/storage/brick1 force
3. [root@clusterFS-node1-salt ~]# gluster volume start gv2
注意：之前平均到gluster1,gluster2复制卷的数据将在卷中丢失，但数据依旧在底层clusterFS-node1-salt.jack.com:/storage/brick1和clusterFS-node2-salt.jack.com:/storage/brick1中，重新添加挂载即可
！！注意：移除卷只能移除复制卷，不能移除条带卷

删除卷：
[root@clusterFS-node1-salt /]# gluster volume delete gv1
！！注意：gv1是分布式复制卷，第一次新建gluster1,gluster2为复制卷，然后加入了gluster3,gluster4复制卷进去，所以以后要想恢复数据必需要先新建gluster1,gluster2为复制卷，然后加入了gluster3,gluster4复制卷进去，顺序不能变，否则会失败。注意在每个/storage/brick1目录下都有gluster的配置文件，不要删除和更忙，应该变个是关系到你能是否恢复数据的关键，对于分布式条带卷来说更至关重要。

添加卷并恢复数据：
1. [root@clusterFS-node1-salt /]# gluster volume create gv1 replica 2 clusterFS-node1-salt.jack.com:/storage/brick1 clusterFS-node2-salt.jack.com:/storage/brick1 force
2. [root@clusterFS-node1-salt /]# gluster volume add-brick gv1 replica 2 clusterFS-node3-salt.jack.com:/storage/brick1 clusterFS-node4-salt.jack.com:/storage/brick1 force
3. [root@clusterFS-node1-salt /]# gluster volume start gv1
4. [root@clusterFS-node1-salt /]# ll -h /gv1/  #看出数据还在
total 36M
-rw-r--r-- 1 root root 9.8M Jan 13 22:10 10M
-rw-r--r-- 1 root root  20M Jan 13 22:11 20M
-rw-r--r-- 1 root root    3 Jan 13 22:11 aa
-rw-r--r-- 1 root root    0 Jan 13 22:17 cc
-rw-r-xr-- 1 root root 6.6M Jan 13 22:29 image123.JPG
-rw-r--r-- 1 root root    3 Jan 13 22:11 tt






 </pre>


