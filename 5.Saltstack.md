#Saltstack深入
####YAML语法：
简介：YAML语法写成sls描述文件，sls全称：Salt State
<pre>
例子：
apache-install:
  pkg.installed:
    - names:
      - httpd
      - httpd-devel
apache-service:
  service.running:
    - name: httpd
    - enable: True

1. apache-service为名称(ID)声明,不写names,则默认name就是名称声明的名称。ID声明在高级状态下必须唯一
2. service.running为State声明，状态声明，模块声明
3. - name: httpd为名称选项声明

实际应用例子：LAMP架构
1. 安装软件包		pkg
2. 修改配置文件	file
3. 启动服务 		service

pkg.installed	安装
pkg.latest	确保最新版本软件
pkg.remove	卸载
pkg.purge	卸载并删除配置文件

1.同时安装多个包：
common_packages:
  pkg.installed:
    - pkgs:
      - httpd
      - php
      - mysql
      - mariadb-server
      - php-mysql
      - php-cli
      - php-mbstring

2.配置管理
apache-config:
  file.managed:#一个ID声明中只能是不同的状态模块，因为会冲突
    - name: /etc/httpd/conf/httpd.conf #要被配置的文件
    - source: salt://files/httpd.conf #源文件在哪
    - user: root  #文件所有者用户
    - group: root  #文件所有者组
    - mode: 644  #文件权限

php-config:
  file.managed:
    - name: /etc/php.ini
    - source: salt://files/php.ini
    - user: root
    - group: root
    - mode: 644

mysql-config:
   file.managed:
    - name: /etc/my.cnf
    - source: salt://files/my.cnf
    - user: root
    - group: root
    - mode: 644

3.服务管理
apache-service:
  service.running:
    - name: httpd
    - enable: True
    - reload: True

mysql-service:
  service.running:
    - name: mariadb
    - enable: True
    - reload: True
注：salt://这个根目录是当前base环境下的根目录/srv/salt，如果是其他环境时，则寻找其他环境时的根目录

#vim /srv/salt/lamp/lamp.sls
lamp-pkg:
  pkg.installed:
    - pkgs:
      - httpd
      - php
      - mariadb
      - mariadb-server
      - php-mysql
      - php-cli
      - php-mbstring

apache-config:
  file.managed:
    - name: /etc/httpd/conf/httpd.conf
    - source: salt://lamp/files/httpd.conf
    - user: root
    - group: root
    - mode: 644

php-config:
  file.managed:
    - name: /etc/php.ini
    - source: salt://lamp/files/php.ini
    - user: root
    - group: root
    - mode: 644

mysql-config:
   file.managed:
    - name: /etc/my.cnf
    - source: salt://lamp/files/my.cnf
    - user: root
    - group: root
    - mode: 644

apache-service:
  service.running:
    - name: httpd
    - enable: True
    - reload: True

mysql-service:
  service.running:
    - name: mariadb
    - enable: True
    - reload: True

#or
lamp-pkg:
  pkg.installed:
    - pkgs:
      - httpd
      - php
      - mariadb
      - mariadb-server
      - php-mysql
      - php-cli
      - php-mbstring
     
apache-server:
  file.managed:
    - name: /etc/httpd/conf/httpd.conf
    - source: salt://lamp/files/httpd.conf
    - user: root
    - group: root
    - mode: 644
  service.running:
    - name: httpd
    - enable: True
    - reload: True

mysql-server:
   file.managed:
    - name: /etc/my.cnf
    - source: salt://lamp/files/my.cnf
    - user: root
    - group: root
    - mode: 644
  service.running:
    - name: mariadb
    - enable: True
    - reload: True

php-config:
  file.managed:
    - name: /etc/php.ini
    - source: salt://lamp/files/php.ini
    - user: root
    - group: root
    - mode: 644
salt 'linux*' state.sls lamp.lamp

状态间关系：
 1. 我依赖谁    require: 
    require: #我依赖下面两个模块的id是否为True，为True我才能成功执行
      - pkg: lamp-pkg  #pkg是模块名，后面是相关的ID
      - file: apache-config
 2. 我被谁依赖   require_in
    require_in: mysql-service #我被其他模块依赖，我必须先成功执行，其他依赖我的模块才能成功执行
 3. 我监控谁  watch，监控salt://lamp/files/httpd.conf配置文件是否被更改，如果被更改则重载配置文件到各个minion
  - reload: True#写了就是重载配置文件，不写这个就是重启服务 
  - watch:
      - file: apache-config #watch包括require,也就是apache-config首先存在，然后apache-config被更改了才用reload进行重载
 4. 我被谁监控 watch_in
 5. 我引用谁  include
 include: 
   - lamp.mysql
   - lamp.apache
 6. 我扩展谁
编写SLS技巧：
1. 按状态分类，如果单独使用，很清晰
2. 按服务分类，可以被其他的SLS include。例如LNMP的sls include mysql的sls
例：
lamp-pkg:
  pkg.installed:
    - pkgs:
      - httpd
      - php
      - mariadb
      - mariadb-server
      - php-mysql
      - php-cli
      - php-mbstring

apache-config:
  file.managed:
    - name: /etc/httpd/conf/httpd.conf
    - source: salt://lamp/files/httpd.conf
    - user: root
    - group: root
    - mode: 644
    
apache-service:
  service.running:
    - name: httpd
    - enable: True
    - reload: True
    - require:   #依赖关系 
      - pkg: lamp-pkg  
    - watch:
      - file: apache-config
 
mysql-config:
   file.managed:
    - name: /etc/my.cnf
    - source: salt://lamp/files/my.cnf
    - user: root
    - group: root
    - mode: 644
    - require_in: mysql-service
  
mysql-service:
  service.running:
    - name: mariadb
    - enable: True
    - reload: True
</pre>

###Jinja模板(python的模板语言)
<pre>
两种分隔符:
1 {%……%}条件语句
2 {{……}}变量
使用一个模板需要3步走：
1. 告诉File模块，使用Jinja模板，这样jinja格式就可以生效了
- template: jinja
2. 你要列出参数列表
- defaults:
  PORT: 88  #指定POST变量值为88
3. 模板引用
{{ PORT }} #在source文件中添加jinja格式的变量

例:
apache-config:
  file.managed:
    - name: /etc/httpd/conf/httpd.conf
    - source: salt://lamp/files/httpd.conf
    - user: root
    - group: root
    - mode: 644
    - template: jinja
    - defaults:
      PORT: 99

#jinja模板可以使用：salt grains pillar 赋值

grains目标选择fqdn_ip4这个item是跟minion的主机名绑定的，是唯一的，vim /etc/hosts 后，使用 `salt '*' saltutil.sync_grains` 直接同步即可
FQDN:完全合格域名

一、写在模板文件中
使用grains进行赋值：
Listen {{grains['fqdn_ip4'][0]}}:{{ PORT }}
Salt远程执行模块：
{{ salt['network.hw_addr']('eth0') }}
#一样效果：取eth0的mac地址：salt '*' network.hw_addr eth0
使用pillar赋值：{{ pillar['apache']}}
二、写在SLS文件里面的Defaults变量列表中
- defaults:
    IPADDR: {{ grains['fqdn_ip4'][0] }}
    PORT: 88

混合匹配：and,or,not
戴明环又叫质量环（PDCA(plan do check action)）
yum install lrzsz -y 
rz:上传    sz：下载

salt实战：
首先头脑风暴：
1. 系统初始化
2. 功能模板：设置单独的目录，haproxy,nginx,php,mysql,memcached,尽可能的全、独立
3. 业务模块：根据业务类型划分，例如web服务。论坛bbs，然后用include包括进来
干活：
salt环境配置：开发、测试（功能测试、性能测试）、预生产、生产
1. base基础环境
init目录，环境初始化（在salt基础上）：1.dns配置，2.history记录时间3.记录命令操作4.内核参数优化5.安装yum仓库6.安装zabbix-agent

2. prod生产环境

步骤：
1. vim /etc/salt/master
file_roots:
  base:
    - /srv/salt/base
  prod:
    - /srv/salt/prod
pillar_roots:
  base:
    - /srv/pillar/base
  prod:
    - /srv/pillar/prod

[root@SaltstackServer /srv/salt]# mkdir base 
[root@SaltstackServer /srv/salt]# mkdir prod
[root@SaltstackServer /srv/pillar]# mkdir base
[root@SaltstackServer /srv/pillar]# mkdir prod
[root@SaltstackServer /srv/pillar]# systemctl restart salt-master.service 
1.base基础环境初始化编写基础模块sls
#dns.sls----dns配置
[root@SaltstackServer /srv/salt/base/init]# cat dns.sls
/etc/resolve.conf:
  file.managed:
    - source: salt://init/files/resolv.conf
    - user: root
    - group: root
    - mode: 644
[root@SaltstackServer /srv/salt/base/init/files]# cat resolv.conf
#history.sls----history记录时间
[root@SaltstackServer /srv/salt/base/init]# cat history.sls 
/etc/profile:
  file.append:
    - text:
      - export HISTTIMEFORMAT="%F %T `whoami`"
#audit.sls----记录命令操作
[root@SaltstackServer /srv/salt/base/init]# cat audit.sls 
/etc/bashrc:
  file.append:
    - text:
      - export PROMPT_COMMAND='{ msg=$(history 1 | { read x y; echo $y; });logger "[euid=$(whoami)]":$(whoami):[`pwd`]"$msg"; }'
#sysctl.sls----内核参数优化
[root@SaltstackServer /srv/salt/base/init]# cat sysctl.sls 
net.ipv4.ip_local_port_range:    #客户端打口随机端口范围
  sysctl.present:
    - value: 10000 65000
fs.file-max:
  sysctl.present:
    - value: 2000000
net.ipv4.ip_forward:     #打开ipv4转发
  sysctl.present:
    - value: 1
vm.swappiness:   #swap为0,尽量不要使用swap内存
  sysctl.present:
    - value: 0
#epel.sls----安装yum仓库
[root@SaltstackServer /srv/salt/base/init]# cat epel.sls 
yum_repo_release:
  pkg.installed:
    - sources:
      - epel-release: https://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpm
#zabbix-agent.sls----安装zabbix-agent
[root@SaltstackServer /srv/salt/base/init]# cat zabbix-agent.sls 
zabbix-agent:
  pkg.installed:
    - name: zabbix-agent
  file.managed:
    - name: /etc/zabbix/zabbix_agentd.conf
    - source: salt://init/files/zabbix_agentd.conf
    - template: jinja
    - defaults:
        Zabbix_Server: {{ pillar['Zabbix_Server'] }}
    - require:
      - pkg: zabbix-agent
  service.running:
    - enable: True
    - watch:
      - pkg: zabbix-agent
      - file: zabbix-agent
zabbix_agentd.conf.d:
  file.directory:
    - name: /etc/zabbix/zabbix_agentd.d
    - watch_in: 
      - service: zabbix-agent
    - require:
      - pkg: zabbix-agent


#[root@SaltstackServer /srv/pillar/base]# vim /srv/salt/base/init/files/zabbix_agentd.conf
Server={{ Zabbix_Server }}

#/srv/pillar/base/zabbix/agent.sls--编写pillar
[root@SaltstackServer /srv/pillar/base]# cat zabbix/agent.sls 
Zabbix_Server: 192.168.1.201
#/srv/pillar/base/top.sls--编写top file
[root@SaltstackServer /srv/pillar/base]# cat top.sls 
base:
  '*':
    - zabbix.agent

#init.sls
[root@SaltstackServer /srv/salt/base/init]# cat init.sls 
include:
  - init.dns
  - init.history
  - init.audit
  - init.sysctl
  - init.epel
  - init.zabbix-agent

#top.sls
[root@SaltstackServer /srv/salt/base]# cat top.sls 
base:
  '*':
    - init.init


注意：当你在执行： “salt '*' state.sls init.zabbix-agent ” 时，总共有两个机器，一个机器能执行成功，一个则报错  “ Specified SLS 'zabbix.agent' in environment 'base' is not available on the salt master ”，而这个机器正好是slat-master,实践得出，在salt-master上pillar的base环境下，不能有zabbix.agent这个模块，因为对关键字敏感，最终我把zabbix.agent改成zb.agent后，在使用“salt '*' saltutil.refresh_pillar”同步pillar后可以执行成功

层级关系：
[root@SaltstackServer /srv/salt/base]# tree
.
├── init
│?? ├── audit.sls
│?? ├── dns.sls
│?? ├── epel.sls
│?? ├── files
│?? │?? ├── resolv.conf
│?? │?? └── zabbix_agentd.conf
│?? ├── history.sls
│?? ├── init.sls
│?? ├── sysctl.sls
│?? └── zabbix-agent.sls
└── top.sls
[root@SaltstackServer /srv/salt/base]# salt '*' state.show_top
SaltstackServer.com:
    ----------
    base:
        - init.init
linux-node1:
    ----------
    base:
        - init.init
[root@SaltstackServer /srv/salt/base]# salt '*' state.highstate  --执行高级状态


##haproxy部署
[root@SaltstackServer /srv/salt/prod]# tree

├── cluster
│   ├── files
│   │   ── haproxy-outside.cfg.template
│   └── haproxy-outside.sls
└── modules
    ├── haproxy
    │   ├── files
    │   │   ├── haproxy-1.9.0.tar.gz
    │   │   └── haproxy.init
    │   └── install.sls
    ├── keepalived
    ├── libevent
    ├── memcached
    ├── nginx
    ├── php
    ├── pkg
         ── make.sls


--------------------------------------------------------------
[root@SaltstackServer /srv/salt/prod/modules/pkg]# cat make.sls
make-pkg:
  pkg.installed:
    - pkgs:
      - gcc
      - gcc-c++
      - glibc
      - make
      - autoconf
      - openssl
      - openssl-devel
      - pcre
      - pcre-devel
---------------------------------------------------------------
[root@SaltstackServer /srv/salt/prod/modules/haproxy]# cat install.sls
include:
  - modules.pkg.make

haproxy-install:
  file.managed:
    - name: /usr/local/src/haproxy-1.9.0.tar.gz
    - source: salt://modules/haproxy/files/haproxy-1.9.0.tar.gz
    - mode: 755
    - user: root
    - group: root
  cmd.run:
    - name: cd /usr/local/src && tar -zxf haproxy-1.9.0.tar.gz && cd haproxy-1.9.0 && make TARGET=linux2628 PREFIX=/usr/local/haproxy-1.9.0 && make install PREF                                                                                                                IX=/usr/local/haproxy-1.9.0 && ln -s /usr/local/haproxy-1.9.0 /usr/local/haproxy
    - unless: test -L /usr/local/haproxy
    - require:
      - pkg: make-pkg
      - file: haproxy-install
haproxy-init:
  file.managed:
    - name: /etc/init.d/haproxy
    - source: salt://modules/haproxy/files/haproxy.init
    - mode: 755
    - user: root
    - group: root
    - require_in:
      - file: haproxy-install
  cmd.run:
    - name: chkconfig --add haproxy
    - unless: chkconfig --list | grep haproxy

net.ipv4.ip_nonlocal_bind:
  sysctl.present:
    - value: 1
/etc/haproxy:
  file.directory:
    - user: root
    - group: root
    - mode: 755
-------------------------------------------------------------
[root@SaltstackServer /srv/salt/prod/modules/haproxy/files]# cat haproxy.init
#!/bin/sh
#
# chkconfig: - 85 15
# description: HA-Proxy is a TCP/HTTP reverse proxy which is particularly suited \
#              for high availability environments.
# processname: haproxy
# config: /etc/haproxy/haproxy.cfg
# pidfile: /var/run/haproxy.pid

# Script Author: Simon Matter <simon.matter@invoca.ch>
# Version: 2004060600

# Source function library.
if [ -f /etc/init.d/functions ]; then
  . /etc/init.d/functions
elif [ -f /etc/rc.d/init.d/functions ] ; then
  . /etc/rc.d/init.d/functions
else
  exit 0
fi

# Source networking configuration.
. /etc/sysconfig/network

# Check that networking is up.
[ ${NETWORKING} = "no" ] && exit 0

# This is our service name
BASENAME=`basename $0`
if [ -L $0 ]; then
  BASENAME=`find $0 -name $BASENAME -printf %l`
  BASENAME=`basename $BASENAME`
fi

BIN=/usr/local/haproxy/sbin/$BASENAME

CFG=/etc/$BASENAME/$BASENAME.cfg
[ -f $CFG ] || exit 1

PIDFILE=/var/run/$BASENAME.pid
LOCKFILE=/var/lock/subsys/$BASENAME

RETVAL=0

start() {
  quiet_check
  if [ $? -ne 0 ]; then
    echo "Errors found in configuration file, check it with '$BASENAME check'."
    return 1
  fi

  echo -n "Starting $BASENAME: "
  daemon $BIN -D -f $CFG -p $PIDFILE
  RETVAL=$?
  echo
  [ $RETVAL -eq 0 ] && touch $LOCKFILE
  return $RETVAL
}

stop() {
  echo -n "Shutting down $BASENAME: "
  killproc $BASENAME -USR1
  RETVAL=$?
  echo
  [ $RETVAL -eq 0 ] && rm -f $LOCKFILE
  [ $RETVAL -eq 0 ] && rm -f $PIDFILE
  return $RETVAL
}

restart() {
  quiet_check
  if [ $? -ne 0 ]; then
    echo "Errors found in configuration file, check it with '$BASENAME check'."
    return 1
  fi
  stop
  start
}

reload() {
  if ! [ -s $PIDFILE ]; then
    return 0
  fi

  quiet_check
  if [ $? -ne 0 ]; then
    echo "Errors found in configuration file, check it with '$BASENAME check'."
    return 1
  fi
  $BIN -D -f $CFG -p $PIDFILE -sf $(cat $PIDFILE)
}

check() {
  $BIN -c -q -V -f $CFG
}

quiet_check() {
  $BIN -c -q -f $CFG
}

rhstatus() {
  status $BASENAME
}

condrestart() {
  [ -e $LOCKFILE ] && restart || :
}

# See how we were called.
case "$1" in
  start)
    start
    ;;
  stop)
    stop
    ;;
  restart)
    restart
    ;;
  reload)
    reload
    ;;
  condrestart)
    condrestart
    ;;
  status)
    rhstatus
    ;;
  check)
    check
    ;;
  *)
    echo $"Usage: $BASENAME {start|stop|restart|reload|condrestart|status|check}"
    exit 1
esac

exit $?
-----------------------------------------------------------
[root@SaltstackServer /srv/salt/prod/cluster]# cat haproxy-outside.sls
include:
  - modules.haproxy.install

haproxy-service:
  file.managed:
    - name: /etc/haproxy/haproxy.cfg
    - source: salt://cluster/files/haproxy-outside.cfg.template
    - user: root
    - group: root
    - mode: 644
  service.running:
    - name: haproxy
    - enable: True
    - reload: True
    - require:
      - cmd: haproxy-install
    - watch:
      - file: haproxy-service
--------------------------------------------------------------------------
[root@SaltstackServer /srv/salt/prod/cluster/files]# cat haproxy-outside.cfg.template
global
maxconn 100000
chroot /usr/local/haproxy
uid 99
gid 99
daemon
nbproc 1
pidfile /usr/local/haproxy/logs/haproxy.pid
log 127.0.0.1 local3 info

#默认参数设置
defaults
option http-keep-alive
maxconn 100000
mode http
timeout connect 5000ms
timeout client  50000ms
timeout server 50000ms

#开启Haproxy Status状态监控，增加验证
listen stats
mode http
bind 0.0.0.0:8888
stats enable
stats uri     /haproxy-status
stats auth    haproxy:saltstack

#前端设置
frontend frontend_www_example_com
bind 192.168.1.236:80
mode http
option httplog
log global
    default_backend backend_www_example_com

#后端设置
backend backend_www_example_com
option forwardfor header X-REAL-IP
option httpchk HEAD / HTTP/1.0
balance source
server web-node1  192.168.1.231:8080 check inter 2000 rise 30 fall 15
server web-node2  192.168.1.232:8080 check inter 2000 rise 30 fall 15
---------------------------------------------------------


##keepalived部署：
[root@SaltstackServer /srv/salt/prod]# tree
.
├── cluster
│   ├── files
│   │   ├── haproxy-outside.cfg.template
│   │   └── haproxy-outside-keepalived.conf
│   ├── haproxy-outside-keepalived.sls
│   └── haproxy-outside.sls
└── modules
    ├── haproxy
    │   ├── files
    │   │   ├── haproxy-1.9.0.tar.gz
    │   │   └── haproxy.init
    │   └── install.sls
    ├── keepalived
    │   ├── files
    │   │   ├── keepalived-1.2.17.tar.gz
    │   │   ├── keepalived.init
    │   │   └── keepalived.sysconfig
    │   └── install.sls
    ├── libevent
    ├── nginx
    ├── php
    ├── pkg
        └── make.sls

---------------------------------------------------------
[root@SaltstackServer /srv/salt/prod/modules/keepalived]# cat install.sls
{% set keepalived_tar= 'keepalived-1.2.17.tar.gz'  %}
{% set keepalived_source= 'salt://modules/keepalived/files/'  %}

keepalived-install:
  file.managed:
    - name: /usr/local/src/{{ keepalived_tar }}
    - source: {{ keepalived_source }}{{ keepalived_tar }}
    - mode: 755
    - user: root
    - group: root
  cmd.run:
    - name: cd /usr/local/src && tar zxf {{  keepalived_tar }} && cd keepalived-1.2.17 && ./configure --prefix=/usr/local/keepalived --disable-fwmark && make && make install
    - unless: test -d /usr/local/keepalived
    - require:
      - file: keepalived-install

/etc/sysconfig/keepalived:
  file.managed:
    - source: {{ keepalived_source }}keepalived.sysconfig
    - mode: 644
    - user: root
    - group: root

/etc/init.d/keepalived:
  file.managed:
    - source: {{ keepalived_source }}keepalived.init
    - mode: 755
    - user: root
    - group: root

keepalived-init:
  cmd.run:
    - name: chkconfig --add keepalived
    - unless: chkconfig --list | grep keepalived
    - require:
      - file: /etc/init.d/keepalived

/etc/keepalived:
  file.directory:
    - user: root
    - group: root

---------------------------------------------------------
[root@SaltstackServer /srv/salt/prod/modules/keepalived]# cat files/keepalived.sysconfig
# Options for keepalived. See `keepalived --help' output and keepalived(8) and
# keepalived.conf(5) man pages for a list of all options. Here are the most
# common ones :
#
# --vrrp               -P    Only run with VRRP subsystem.
# --check              -C    Only run with Health-checker subsystem.
# --dont-release-vrrp  -V    Dont remove VRRP VIPs & VROUTEs on daemon stop.
# --dont-release-ipvs  -I    Dont remove IPVS topology on daemon stop.
# --dump-conf          -d    Dump the configuration data.
# --log-detail         -D    Detailed log messages.
# --log-facility       -S    0-7 Set local syslog facility (default=LOG_DAEMON)
#

KEEPALIVED_OPTIONS="-D"
---------------------------------------------------------   
[root@SaltstackServer /srv/salt/prod/modules/keepalived]# cat files/keepalived.init
#!/bin/sh
#
# Startup script for the Keepalived daemon
#
# processname: keepalived
# pidfile: /var/run/keepalived.pid
# config: /etc/keepalived/keepalived.conf
# chkconfig: - 21 79
# description: Start and stop Keepalived

# Source function library
. /etc/rc.d/init.d/functions

# Source configuration file (we set KEEPALIVED_OPTIONS there)
. /etc/sysconfig/keepalived

RETVAL=0

prog="keepalived"

start() {
    echo -n $"Starting $prog: "
#    daemon keepalived ${KEEPALIVED_OPTIONS}
    daemon /usr/local/keepalived/sbin/keepalived ${KEEPALIVED_OPTIONS}
    RETVAL=$?
    echo
    [ $RETVAL -eq 0 ] && touch /var/lock/subsys/$prog
}

stop() {
    echo -n $"Stopping $prog: "
    killproc keepalived
    RETVAL=$?
    echo
    [ $RETVAL -eq 0 ] && rm -f /var/lock/subsys/$prog
}

reload() {
    echo -n $"Reloading $prog: "
    killproc keepalived -1
    RETVAL=$?
    echo
}

# See how we were called.
case "$1" in
    start)
        start
        ;;
    stop)
        stop
        ;;
    reload)
        reload
        ;;
    restart)
        stop
        start
        ;;
    condrestart)
        if [ -f /var/lock/subsys/$prog ]; then
            stop
            start
        fi
        ;;
    status)
        status keepalived
        RETVAL=$?
        ;;
    *)
        echo "Usage: $0 {start|stop|reload|restart|condrestart|status}"
        RETVAL=1
esac

exit $RETVAL
---------------------------------------------------------
[root@SaltstackServer /srv/salt/prod/cluster]# cat haproxy-outside-keepalived.sls
include:
  - modules.keepalived.install

keepalived-server:
  file.managed:
    - name: /etc/keepalived/keepalived.conf
    - source: salt://cluster/files/haproxy-outside-keepalived.conf
    - mode: 644
    - user: root
    - group: root
    - template: jinja
    {% if grains['fqdn'] == 'SaltstackServer.com' %}
    - ROUTEID: haproxy_ha
    - STATEID: MASTER
    - PRIORITYID: 150
    {% elif grains['fqdn'] == 'linux-node1' %}
    - ROUTEID: haproxy_ha
    - STATEID: BACKUP
    - PRIORITYID: 100
    {% endif %}
  service.running:
    - name: keepalived
    - enable: True
    - watch:
      - file: keepalived-server
---------------------------------------------------------
[root@SaltstackServer /srv/salt/prod/cluster/files]# cat haproxy-outside-keepalived.conf
! Configuration File for keepalived
global_defs {
   notification_email {
     saltstack@example.com
   }
   notification_email_from keepalived@example.com
   smtp_server 127.0.0.1
   smtp_connect_timeout 30
   router_id {{ROUTEID}}
}

vrrp_instance haproxy_ha {
state {{STATEID}}
interface eth0
    virtual_router_id 36
priority {{PRIORITYID}}
    advert_int 1
authentication {
auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
       192.168.1.236
    }
}

---------------------------------------------------------
[root@SaltstackServer /srv/salt/base]# cat top.sls
base:
  '*':
    - init.init
prod:
  '*':
    - cluster.haproxy-outside
    - cluster.haproxy-outside-keepalived
---------------------------------------------------------
[root@SaltstackServer /srv/salt/prod/cluster/files]# salt -L 'SaltstackServer.com,linux-node1' state.highstate






</pre>